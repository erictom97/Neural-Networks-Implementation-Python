{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions introduce non-linearity to the neural network. They are used to transform the input signal into the output signal. The output signal is used to make predictions.\n",
    "\n",
    "In ReLU, the output is 0 if the input is less than 0, and the output is the input if the input is greater than 0. The ReLU function is defined as:\n",
    "f(x) = max(0, x)\n",
    "\n",
    "Hidden layer Activation works as follows:\n",
    " - the input weights determines the slope of the RELU function. If the input weights are large, the slope of the RELU function is steep. If the input weights are small, the slope of the RELU function is shallow.\n",
    "\n",
    " - The bias controls the activation point\n",
    "\n",
    " - if we negate the weights, the slope of the RELU function is also negated determining when the neuron should be deactivated\n",
    "\n",
    " - connecting this neuron to another neuron and adjusting its bias, the function is shifted vertically(up or down)\n",
    "\n",
    " - if we negate the weights of the 2nd neuron, we have a lower and upper bound for the input signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
